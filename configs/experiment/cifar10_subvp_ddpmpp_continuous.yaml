# @package _global_

# to execute this experiment run:
# python run.py experiment=fixedpoint.yaml

defaults:
  - override /mode: exp.yaml
  - override /dataset: cifar10.yaml
  - override /training: default_cifar10.yaml
  - override /model: cifarnet.yaml
  - override /sde: subvp.yaml
  - override /loss: default.yaml
  # - override /callbacks: null
  # - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "${dataset.ds_name}_${sde.name}_ddpmpp_continuous_${training.seed}"

sde:
  continuous: True
  smallest_time: 1e-2

  sampling:
    smallest_time: 1e-2

dataset:
  centered: True

loss:
  reduce_mean: True

model:
  embedding_type: 'positional'


# callbacks:
#   model_checkpoint:
#     _target_: pytorch_lightning.callbacks.ModelCheckpoint
#     monitor: "val/acc"
#     mode: "max"
#     save_top_k: 4
#     save_last: True
#     verbose: False
#     dirpath: "checkpoints/"
#     filename: "epoch_{epoch:03d}"
#     auto_insert_metric_name: False
#   rich_progress_bar:
#     _target_: pytorch_lightning.callbacks.RichProgressBar
