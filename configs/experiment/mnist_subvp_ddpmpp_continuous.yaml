# @package _global_

# to execute this experiment run:
# python run.py experiment=fixedpoint.yaml

defaults:
  - override /mode: exp.yaml
  - override /main/dataset: mnist.yaml
  - override /main/training: default_cifar10.yaml
  - override /main/model: mnistnet.yaml
  - override /main/sde: subvp.yaml
  - override /main/loss: default.yaml
  # - override /callbacks: null
  # - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "${main.dataset.ds_name}_${main.sde.name}_ddpmpp_continuous_${main.training.seed}"
main:
  sde:
    continuous: True
    smallest_time: 1e-2

    sampling:
      smallest_time: 1e-2

  dataset:
    centered: True

  loss:
    reduce_mean: True

  model:
    embedding_type: 'positional'


# callbacks:
#   model_checkpoint:
#     _target_: pytorch_lightning.callbacks.ModelCheckpoint
#     monitor: "val/acc"
#     mode: "max"
#     save_top_k: 4
#     save_last: True
#     verbose: False
#     dirpath: "checkpoints/"
#     filename: "epoch_{epoch:03d}"
#     auto_insert_metric_name: False
#   rich_progress_bar:
#     _target_: pytorch_lightning.callbacks.RichProgressBar
